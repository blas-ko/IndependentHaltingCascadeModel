{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65bf3fbb-de14-445b-9611-98086534a7c5",
   "metadata": {},
   "source": [
    "# Heterogeneous IHC model\n",
    "Hetergeneity comes from giving each agents a unique skillset and computing probabilities based on their fitness with a given job vacancy.\n",
    "\n",
    "We also make experiments for the Direct Recommender system and homophilic networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7c8cb6e-d00b-43f7-b4f4-9e4fb18bb7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68ea44ba-a4d8-47d1-9618-bf9f6bc3cb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import poisson\n",
    "# parallel computing\n",
    "from itertools import product\n",
    "from functools import partial\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# local libraries\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from model import IndependentHaltingCascade as IHC\n",
    "from network_generators.erdos_renyi import generate_erdos_renyi_edgelist\n",
    "from network_generators.oracle import generate_oracle\n",
    "from network_generators.homophilic import generate_homophilic_network_edgelist, onehot_encode_skills\n",
    "from skills_and_vacancies import sample_application_and_hiring_probs, sample_skills, sample_skills_correlated, majority_groups\n",
    "from utils.simulation_helpers import run_model, perform_independent_model_simulations, save_experiment\n",
    "import utils.simulation_helpers as sh\n",
    "from utils.graphtool_reader import import_graphtool_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "206b110c-b0ec-4ccf-9cbc-45e74106269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global parameters\n",
    "# Network\n",
    "N_NODES = 2000\n",
    "K_AVG = 20\n",
    "ORACLE_REACH = 0.5\n",
    "ORACLE_DEGREE = round(N_NODES * ORACLE_REACH)\n",
    "ER_GENERATOR = lambda: generate_erdos_renyi_edgelist(n_nodes=N_NODES, prob_edge=K_AVG/N_NODES)\n",
    "BA_GENERATOR = lambda: generate_barabasi_albert(N_NODES, K_AVG)\n",
    "ORACLE_GENERATOR = lambda: generate_oracle(N_NODES, ORACLE_DEGREE)\n",
    "\n",
    "# IHC (with skills)\n",
    "AVG_SKILLS = 3\n",
    "CIs = [0.5, 0.9, 0.99] # confidence intervals\n",
    "VACANCY_REQUIREMENTS = [int(poisson.interval(CI, AVG_SKILLS)[1]) for CI in CIs] # [4,6,8]\n",
    "\n",
    "# IHC (homogeneous)\n",
    "P_RECOMMENDATION = 0.1\n",
    "P_APPLICATION = 0.25 # same as Milgram's dropout rate\n",
    "P_HIRING = 0.1\n",
    "\n",
    "# Homophily and skill correlations\n",
    "ALPHA = 0.5 # Dirichlet param. for correlated skills (0.1 => very correlated, 5 => very uncorrelated)\n",
    "N_SKILL_GROUPS = 3\n",
    "HOMOPHILY = 6 # (1 => low homophily, 6 => high homophily, np.inf => extreme homophily)\n",
    "\n",
    "# Real networks\n",
    "REAL_NETWORK_SELECTION = [\n",
    "    \"copenhagen/sms\", # directed, phone sms, n=568, m=24k\n",
    "    \"uni_email\", # directed, email exchanges, n=1.1k, m=10k\n",
    "    \"twitter_15m\", # directed, followerships, n=87k, m=6M\n",
    "]\n",
    "\n",
    "# grid\n",
    "N_SIMULATIONS = 200\n",
    "NEW_EDGELIST_EVERY = N_SIMULATIONS // 10\n",
    "\n",
    "# saving\n",
    "PATH = '../results/data/heterogeneous'\n",
    "# FN_SUFFIX = 'n_nodes={n_nodes}-k_avg={k_avg}_ph={ph}.npy'\n",
    "FN_SUFFIX = 'n_nodes={n_nodes}-k_avg={k_avg}_pr={pr}-avg_skills={avg_skills}-requirements={requirements}.npy'\n",
    "FN_SUFFIX_CORR = 'n_nodes={n_nodes}-k_avg={k_avg}_pr={pr}-avg_skills={avg_skills}-requirements={requirements}-alpha={alpha}-n_skillgroups={n_skillgroups}_homophily={homophily}.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aa536bf-649d-40c2-9a37-0a1d9bb489ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Skill-specific model runner\n",
    "def run_model_skills(edgelist, λ, nν, n_skills=None, nodelist=None, **kw):\n",
    "    if nodelist is None:\n",
    "        nodelist = list( set(edgelist['source']).union(edgelist['target']) )\n",
    "    pa, ph = sample_application_and_hiring_probs(nodelist, λ, nν, n_skills)\n",
    "    return run_model(edgelist, application_probs=pa, hiring_probs=ph, **kw)\n",
    "\n",
    "### Oracle-specific model runner\n",
    "def run_model_oracle_skills(edgelist, λ, nν, n_skills=None, nodelist=None, **kw):\n",
    "    if nodelist is None:\n",
    "        nodelist = list( set(edgelist['source']).union(edgelist['target']) )    \n",
    "    _, ph = sample_application_and_hiring_probs(nodelist, λ, nν, n_skills)    \n",
    "    return run_model(edgelist, application_probs=1, hiring_probs=ph, initial_spreaders=['oracle'], **kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db5ae1e-6cd9-4954-95d6-f718a33b0684",
   "metadata": {},
   "source": [
    "# IHC vs Oracle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f48dcf-a208-4a17-b4c6-a8405bde69f8",
   "metadata": {},
   "source": [
    "## Erdos-Renyi networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00b68406-a9a2-4f06-81e3-df0e6484cf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL: 150 tasks per model...\n",
      "IHCM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  2.9min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oracle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:   31.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  1.4min finished\n"
     ]
    }
   ],
   "source": [
    "### SIMULATIONS \n",
    "np.random.seed(42) # for reproducibility\n",
    "\n",
    "# grid parameters\n",
    "grid_resolution = 50\n",
    "p_recs = np.linspace(0.001, 0.999, num=grid_resolution)[::-1] \n",
    "\n",
    "# Partial functions\n",
    "# IHC model\n",
    "f_ihc = partial(perform_independent_model_simulations, \n",
    "            n_simulations = N_SIMULATIONS,\n",
    "            new_edgelist_every = NEW_EDGELIST_EVERY,\n",
    "            edgelist_generator = ER_GENERATOR,\n",
    "            λ = AVG_SKILLS,\n",
    "            run_model_func = run_model_skills,\n",
    "            nodelist = range(N_NODES),\n",
    "            )\n",
    "# Oracle\n",
    "f_oracle = partial(perform_independent_model_simulations, \n",
    "            n_simulations = N_SIMULATIONS,\n",
    "            new_edgelist_every = NEW_EDGELIST_EVERY,\n",
    "            edgelist_generator = ORACLE_GENERATOR,\n",
    "            λ = AVG_SKILLS,\n",
    "            run_model_func = run_model_oracle_skills,\n",
    "            nodelist = range(N_NODES),\n",
    "            )\n",
    "\n",
    "## Perform experiments in parallel\n",
    "print(f\"TOTAL: {grid_resolution*len(VACANCY_REQUIREMENTS)} tasks per model...\")\n",
    "parallel = Parallel(n_jobs=-1, prefer=\"processes\", verbose=5)  # use all cores\n",
    "\n",
    "print(\"IHCM\")\n",
    "vals_ihc = parallel(\n",
    "    delayed(f_ihc)(recommendation_probs=pr, nν=nν)\n",
    "    for pr in p_recs\n",
    "    for nν in VACANCY_REQUIREMENTS\n",
    ")\n",
    "print(\"Oracle\")\n",
    "vals_oracle = parallel(\n",
    "    delayed(f_oracle)(recommendation_probs=pr, nν=nν)\n",
    "    for pr in p_recs\n",
    "    for nν in VACANCY_REQUIREMENTS\n",
    ")\n",
    "\n",
    "# Unzip values into different result diagnostics\n",
    "depths_dist, sizes_tot_dist, sizes_dist, successes_dist = zip(*vals_ihc)\n",
    "depths_oracle_dist, sizes_tot_oracle_dist, sizes_oracle_dist, successes_oracle_dist = zip(*vals_oracle)\n",
    "\n",
    "## Save\n",
    "# IHC\n",
    "fn_suffix = f'n_nodes={N_NODES}-k_avg={K_AVG}-avg_skills={AVG_SKILLS}.npy'\n",
    "fn_depth      = os.path.join( PATH, 'pr_vs_requirements', 'ER', 'chain_depths', fn_suffix )\n",
    "fn_applicants = os.path.join( PATH, 'pr_vs_requirements', 'ER', 'applicants', fn_suffix )\n",
    "fn_size       = os.path.join( PATH, 'pr_vs_requirements', 'ER', 'chain_size', fn_suffix )\n",
    "fn_success    = os.path.join( PATH, 'pr_vs_requirements', 'ER', 'success_rate', fn_suffix )\n",
    "save_experiment(fn_depth, depths_dist, p_recs, VACANCY_REQUIREMENTS)\n",
    "save_experiment(fn_applicants, sizes_dist, p_recs, VACANCY_REQUIREMENTS)\n",
    "save_experiment(fn_size, sizes_tot_dist, p_recs, VACANCY_REQUIREMENTS)\n",
    "save_experiment(fn_success, successes_dist, p_recs, VACANCY_REQUIREMENTS)\n",
    "\n",
    "# Oracle\n",
    "fn_suffix_oracle = f'n_nodes={N_NODES}-k_avg={ORACLE_DEGREE}-avg_skills={AVG_SKILLS}'\n",
    "fn_oracle_depth      = os.path.join( PATH, 'pr_vs_requirements', 'oracle', 'chain_depths', fn_suffix_oracle )\n",
    "fn_oracle_applicants = os.path.join( PATH, 'pr_vs_requirements', 'oracle', 'applicants', fn_suffix_oracle )\n",
    "fn_oracle_size       = os.path.join( PATH, 'pr_vs_requirements', 'oracle', 'chain_size', fn_suffix_oracle )\n",
    "fn_oracle_success    = os.path.join( PATH, 'pr_vs_requirements', 'oracle', 'success_rate', fn_suffix_oracle )\n",
    "save_experiment(fn_oracle_depth, depths_oracle_dist, p_recs, VACANCY_REQUIREMENTS)\n",
    "save_experiment(fn_oracle_applicants, sizes_oracle_dist, p_recs, VACANCY_REQUIREMENTS)\n",
    "save_experiment(fn_oracle_size, sizes_tot_oracle_dist, p_recs, VACANCY_REQUIREMENTS)\n",
    "save_experiment(fn_oracle_success, successes_oracle_dist, p_recs, VACANCY_REQUIREMENTS)\n",
    "\n",
    "print(\"Saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290b854d-d3bb-47f8-b916-288ce788532e",
   "metadata": {},
   "source": [
    "## Homophilic networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c134be7c-7b04-4db4-b508-719fe1a90a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_homophilic(n_nodes, k_avg, λ, nν, n_skill_groups=2, alpha=0.5, homophily=4, edgelist=None, n_skills=None, nodelist=None, **kw):\n",
    "    if nodelist is None:\n",
    "            nodelist = np.arange(n_nodes)\n",
    "    pa, ph, skills = sample_application_and_hiring_probs(nodelist, λ, nν, n_skills=n_skills, n_groups=n_skill_groups, alpha=alpha, return_skills=True)\n",
    "\n",
    "    # zero-homophily should be ER network\n",
    "    if homophily == 0:\n",
    "        edgelist = generate_erdos_renyi_edgelist(n_nodes, prob_edge=k_avg/n_nodes)\n",
    "    else:\n",
    "        edgelist = generate_homophilic_network_edgelist(skills, k_avg, homophily)        \n",
    "\n",
    "    results = run_model(edgelist, application_probs=pa, hiring_probs=ph, return_chain_ends=True, **kw)\n",
    "    chain_depth, chain_size, applicants, is_hired, chain_ends_nodes = results\n",
    "    \n",
    "    # Check if initial spreader and hired node are in the same skill group (homophilic hire)     \n",
    "    initial_spreader, hired_node = chain_ends_nodes\n",
    "    is_hire_in_spreader_group = False\n",
    "    if hired_node is not None:\n",
    "        initial_spreader, hired_node = initial_spreader[0], hired_node[0]\n",
    "        skill_groups = majority_groups(skills, n_skill_groups)\n",
    "        is_hire_in_spreader_group = skill_groups.loc[initial_spreader] == skill_groups.loc[hired_node]\n",
    "            \n",
    "    return chain_depth, chain_size, applicants, is_hired, is_hire_in_spreader_group "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b4c5b9-eabd-43bc-bdea-0ce8651928c4",
   "metadata": {},
   "source": [
    "### homophily vs recommendation prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee60d64a-0827-4c3b-b4e0-45de83387b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL: 300 tasks per model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   27.5s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed: 28.8min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed: 74.4min\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed: 139.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 153.4min finished\n"
     ]
    }
   ],
   "source": [
    "### SIMULATIONS \n",
    "np.random.seed(42) # for reproducibility\n",
    "\n",
    "# grid parameters\n",
    "grid_resolution = 50\n",
    "p_recs = np.linspace(0.001, 0.999, num=grid_resolution)[::-1] \n",
    "homophilies = [0, 2, 4, 8, 12, np.inf]\n",
    "\n",
    "# skills parameters\n",
    "nν = 6 # 0.9 CI\n",
    "\n",
    "# Partial function\n",
    "f = partial(perform_independent_model_simulations, \n",
    "            n_simulations = N_SIMULATIONS * 2,\n",
    "            new_edgelist_every = NEW_EDGELIST_EVERY,\n",
    "            run_model_func = run_model_homophilic,\n",
    "            λ = AVG_SKILLS,\n",
    "            nν = nν, # num vacancy requirements\n",
    "            alpha = ALPHA,\n",
    "            n_skill_groups = N_SKILL_GROUPS,\n",
    "            n_nodes = N_NODES,\n",
    "            k_avg = K_AVG,\n",
    "            return_chain_ends = True,\n",
    "            )\n",
    "\n",
    "## Perform experiments in parallel\n",
    "print(f\"TOTAL: {grid_resolution*len(homophilies)} tasks per model...\")\n",
    "parallel = Parallel(n_jobs=-1, prefer=\"processes\", verbose=10)  # use all cores\n",
    "vals = parallel(\n",
    "    delayed(f)(recommendation_probs=pr, homophily=homophily)\n",
    "    for pr in p_recs\n",
    "    for homophily in homophilies\n",
    ")\n",
    "\n",
    "# Unzip values into different result diagnostics\n",
    "depths_dist, sizes_tot_dist, sizes_dist, successes_dist, homophilic_hire_dist = zip(*vals)\n",
    "\n",
    "# ## Save\n",
    "fn_suffix = f'n_nodes={N_NODES}-k_avg={K_AVG}-avg_skills={AVG_SKILLS}-n_skill_groups={N_SKILL_GROUPS}-alpha={ALPHA}.npy'\n",
    "fn_depth      = os.path.join( PATH, 'pr_vs_homophily', 'homophilic', 'chain_depths', fn_suffix )\n",
    "fn_applicants = os.path.join( PATH, 'pr_vs_homophily', 'homophilic', 'applicants', fn_suffix )\n",
    "fn_size       = os.path.join( PATH, 'pr_vs_homophily', 'homophilic', 'chain_size', fn_suffix )\n",
    "fn_success    = os.path.join( PATH, 'pr_vs_homophily', 'homophilic', 'success_rate', fn_suffix )\n",
    "fn_homo       = os.path.join( PATH, 'pr_vs_homophily', 'homophilic', 'homophilic_hire', fn_suffix )\n",
    "save_experiment(fn_depth, depths_dist, p_recs, homophilies)\n",
    "save_experiment(fn_applicants, sizes_dist, p_recs, homophilies)\n",
    "save_experiment(fn_size, sizes_tot_dist, p_recs, homophilies)\n",
    "save_experiment(fn_success, successes_dist, p_recs, homophilies)\n",
    "save_experiment(fn_homo, homophilic_hire_dist, p_recs, homophilies)\n",
    "\n",
    "print(\"Saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bf40e6-63fb-40a1-a300-bdbad6e23625",
   "metadata": {},
   "source": [
    "### skill correlation vs homophily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d62c27f0-c775-4095-b0f8-4dcd29f5cc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL: 224 tasks per model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   53.7s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed: 18.7min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed: 49.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 224 out of 224 | elapsed: 75.4min finished\n"
     ]
    }
   ],
   "source": [
    "### SIMULATIONS \n",
    "np.random.seed(42) # for reproducibility\n",
    "\n",
    "# grid parameters\n",
    "grid_resolution = 16\n",
    "alphas = np.linspace(0.2, 5, grid_resolution) # 0.1 => very strong correlation, 5 => very weak correlation\n",
    "homophilies = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, np.inf]\n",
    "\n",
    "# skills parameters\n",
    "nν = 6 # 0.9 CI\n",
    "# ihcm parameter\n",
    "pr = 0.2\n",
    "\n",
    "# Partial function\n",
    "f = partial(perform_independent_model_simulations, \n",
    "            n_simulations = N_SIMULATIONS,\n",
    "            new_edgelist_every = NEW_EDGELIST_EVERY,\n",
    "            run_model_func = run_model_homophilic,\n",
    "            λ = AVG_SKILLS,\n",
    "            nν = nν, # num vacancy requirements            \n",
    "            n_skill_groups = N_SKILL_GROUPS,\n",
    "            n_nodes = N_NODES,\n",
    "            k_avg = K_AVG,\n",
    "            recommendation_probs = pr,\n",
    "            return_chain_ends = True,\n",
    "            )\n",
    "\n",
    "## Perform experiments in parallel\n",
    "print(f\"TOTAL: {grid_resolution*len(homophilies)} tasks per model...\")\n",
    "parallel = Parallel(n_jobs=-1, prefer=\"processes\", verbose=5)  # use all cores\n",
    "vals = parallel(\n",
    "    delayed(f)(alpha=alpha, homophily=homophily)\n",
    "    for alpha in alphas\n",
    "    for homophily in homophilies\n",
    ")\n",
    "\n",
    "# Unzip values into different result diagnostics\n",
    "depths_dist, sizes_tot_dist, sizes_dist, successes_dist, homophilic_hire_dist = zip(*vals)\n",
    "\n",
    "# ## Save\n",
    "fn_suffix = f'n_nodes={N_NODES}-k_avg={K_AVG}-avg_skills={AVG_SKILLS}-n_skill_groups={N_SKILL_GROUPS}-alpha={ALPHA}.npy'\n",
    "fn_depth      = os.path.join( PATH, 'alpha_vs_homophily', 'homophilic', 'chain_depths', fn_suffix )\n",
    "fn_applicants = os.path.join( PATH, 'alpha_vs_homophily', 'homophilic', 'applicants', fn_suffix )\n",
    "fn_size       = os.path.join( PATH, 'alpha_vs_homophily', 'homophilic', 'chain_size', fn_suffix )\n",
    "fn_success    = os.path.join( PATH, 'alpha_vs_homophily', 'homophilic', 'success_rate', fn_suffix )\n",
    "fn_homo       = os.path.join( PATH, 'alpha_vs_homophily', 'homophilic', 'homophilic_hire', fn_suffix )\n",
    "save_experiment(fn_depth, depths_dist, alphas, homophilies)\n",
    "save_experiment(fn_applicants, sizes_dist, alphas, homophilies)\n",
    "save_experiment(fn_size, sizes_tot_dist, alphas, homophilies)\n",
    "save_experiment(fn_success, successes_dist, alphas, homophilies)\n",
    "save_experiment(fn_homo, homophilic_hire_dist, alphas, homophilies)\n",
    "\n",
    "print(\"Saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584e374d-1f31-4b28-94ac-0d45ea084b4a",
   "metadata": {},
   "source": [
    "## Real networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd4bf6cf-c29a-41e5-8417-9fde50faaae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 selected networks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('uni_email', 1133, 10903)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reference for more real networks (not used)\n",
    "network_names = [\n",
    "    \"copenhagen/sms\", # directed, phone sms, n=568, m=24k\n",
    "    \"copenhagen/fb_friends\", # undirected, friendships, n=800, m=6.4k\n",
    "    \"copenhagen/calls\", # undirected, phone calls, n=536, m=3.6k\n",
    "    \"copenhagen/bt\", # undirected, blue-tooth proximity, n=692, m=2.4M\n",
    "    \"social_location/brightkite\", # undirected, friendships, n=58k, n=48k\n",
    "    \"social_location/gowalla\", # undirected, friendships, n=196k, n=1.9M\n",
    "    \"twitter\", # directed, followerships, n=465k, m=834k\n",
    "    \"twitter_15m\", # directed, followerships, n=87k, m=6M\n",
    "    \"lastfm_aminer\", # directed, followerships, n=136k, m=1.6M\n",
    "    \"livemocha\", # undirected, friendships, n=104k, m=2.2M\n",
    "    \"douban\", # undirected, friendships, n=154k, m=37k\n",
    "    \"uni_email\", # directed, n=1.1k, m=10k\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e564c1cb-cbcb-455b-a59f-d8ecc94cffae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL: 150 tasks per model...\n",
      "\n",
      "Selected copenhagen/sms network with 568 nodes and 1394 edges.\n",
      " - IHCM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   16.7s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   38.6s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:   51.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Oracle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   24.6s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:   36.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved successfully!\n",
      "\n",
      "Selected uni_email network with 1133 nodes and 10903 edges.\n",
      " - IHCM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   24.7s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Oracle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   17.3s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   39.6s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:   57.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved successfully!\n",
      "\n",
      "Selected twitter_15m network with 87569 nodes and 9416551 edges.\n",
      " - IHCM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "/Users/blaskolic/miniconda3/envs/networks/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 45.2min\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed: 117.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Oracle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 174.3min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed: 19.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 27.8min finished\n"
     ]
    }
   ],
   "source": [
    "### SIMULATIONS\n",
    "np.random.seed(42) # for reproducibility\n",
    "\n",
    "# grid parameters\n",
    "grid_resolution = 50\n",
    "p_recs = np.linspace(0.001, 0.999, num=grid_resolution)[::-1] \n",
    "\n",
    "print(f\"TOTAL: {grid_resolution*len(VACANCY_REQUIREMENTS)} tasks per model...\")\n",
    "parallel = Parallel(n_jobs=-1, prefer=\"processes\", verbose=6)  # use all cores\n",
    "for network_name in REAL_NETWORK_SELECTION:\n",
    "    # import network\n",
    "    edgelist, nodelist = import_graphtool_network(network_name, directed=False)\n",
    "    print(f\"\\nSelected {network_name} network with {len(nodelist)} nodes and {len(edgelist)} edges.\")\n",
    "    \n",
    "    # get degree range for initial spreader\n",
    "    degrees = edgelist.groupby('source').size()\n",
    "    k_min = max(2, degrees.quantile(0.25))\n",
    "    k_max = max(k_min+2, degrees.quantile(0.75))\n",
    "    k0_range = (k_min, k_max)\n",
    "\n",
    "    if network_name == 'twitter_15m':\n",
    "        n_simulations = N_SIMULATIONS // 2\n",
    "    else:\n",
    "        n_simulations = N_SIMULATIONS\n",
    "    \n",
    "    # oracle\n",
    "    def oracle_generator():\n",
    "        # nodes = import_graphtool_network(name, directed=False)[1]\n",
    "        oracle_degree = round(ORACLE_REACH  * len(nodelist))\n",
    "        return generate_oracle(nodelist, oracle_degree)\n",
    "        \n",
    "    # Partial functions\n",
    "    # IHC model\n",
    "    f_ihc = partial(perform_independent_model_simulations, \n",
    "                n_simulations = n_simulations,                \n",
    "                edgelist = edgelist,\n",
    "                λ = AVG_SKILLS,\n",
    "                run_model_func = run_model_skills,\n",
    "                nodelist = nodelist,\n",
    "                k0_range=k0_range,\n",
    "                )\n",
    "    # Oracle\n",
    "    f_oracle = partial(perform_independent_model_simulations, \n",
    "                n_simulations = n_simulations,\n",
    "                new_edgelist_every = NEW_EDGELIST_EVERY,\n",
    "                edgelist_generator = oracle_generator,\n",
    "                λ = AVG_SKILLS,\n",
    "                run_model_func = run_model_oracle_skills,\n",
    "                nodelist = nodelist,\n",
    "                )\n",
    "    \n",
    "    ## Perform experiments in parallel    \n",
    "    print(\" - IHCM\")\n",
    "    vals_ihc = parallel(\n",
    "        delayed(f_ihc)(recommendation_probs=pr, nν=nν)\n",
    "        for pr in p_recs\n",
    "        for nν in VACANCY_REQUIREMENTS\n",
    "    )\n",
    "    print(\" - Oracle\")\n",
    "    vals_oracle = parallel(\n",
    "        delayed(f_oracle)(recommendation_probs=pr, nν=nν)\n",
    "        for pr in p_recs\n",
    "        for nν in VACANCY_REQUIREMENTS\n",
    "    )\n",
    "    \n",
    "    # Unzip values into different result diagnostics\n",
    "    depths_dist, sizes_tot_dist, sizes_dist, successes_dist = zip(*vals_ihc)\n",
    "    depths_oracle_dist, sizes_tot_oracle_dist, sizes_oracle_dist, successes_oracle_dist = zip(*vals_oracle)\n",
    "    \n",
    "    ## Save\n",
    "    # IHC\n",
    "    fn_suffix = f'n_nodes={len(nodelist)}-k_avg={round(degrees.mean())}-avg_skills={AVG_SKILLS}.npy'\n",
    "    fn_depth      = os.path.join( PATH, 'pr_vs_requirements', 'real_network', network_name.replace('/','-'), 'chain_depths', fn_suffix )\n",
    "    fn_applicants = os.path.join( PATH, 'pr_vs_requirements', 'real_network', network_name.replace('/','-'), 'applicants', fn_suffix )\n",
    "    fn_size       = os.path.join( PATH, 'pr_vs_requirements', 'real_network', network_name.replace('/','-'), 'chain_size', fn_suffix )\n",
    "    fn_success    = os.path.join( PATH, 'pr_vs_requirements', 'real_network', network_name.replace('/','-'), 'success_rate', fn_suffix )\n",
    "    save_experiment(fn_depth, depths_dist, p_recs, VACANCY_REQUIREMENTS)\n",
    "    save_experiment(fn_applicants, sizes_dist, p_recs, VACANCY_REQUIREMENTS)\n",
    "    save_experiment(fn_size, sizes_tot_dist, p_recs, VACANCY_REQUIREMENTS)\n",
    "    save_experiment(fn_success, successes_dist, p_recs, VACANCY_REQUIREMENTS)\n",
    "    \n",
    "    # Oracle\n",
    "    k_oracle = round( len(nodelist) * ORACLE_REACH )\n",
    "    fn_suffix_oracle = f'n_nodes={len(nodelist)}-k_avg={k_oracle}-avg_skills={AVG_SKILLS}'\n",
    "    fn_oracle_depth      = os.path.join( PATH, 'pr_vs_requirements', 'real_network_oracle', network_name.replace('/','-'), 'chain_depths', fn_suffix_oracle )\n",
    "    fn_oracle_applicants = os.path.join( PATH, 'pr_vs_requirements', 'real_network_oracle', network_name.replace('/','-'), 'applicants', fn_suffix_oracle )\n",
    "    fn_oracle_size       = os.path.join( PATH, 'pr_vs_requirements', 'real_network_oracle', network_name.replace('/','-'), 'chain_size', fn_suffix_oracle )\n",
    "    fn_oracle_success    = os.path.join( PATH, 'pr_vs_requirements', 'real_network_oracle', network_name.replace('/','-'), 'success_rate', fn_suffix_oracle )\n",
    "    save_experiment(fn_oracle_depth, depths_oracle_dist, p_recs, VACANCY_REQUIREMENTS)\n",
    "    save_experiment(fn_oracle_applicants, sizes_oracle_dist, p_recs, VACANCY_REQUIREMENTS)\n",
    "    save_experiment(fn_oracle_size, sizes_tot_oracle_dist, p_recs, VACANCY_REQUIREMENTS)\n",
    "    save_experiment(fn_oracle_success, successes_oracle_dist, p_recs, VACANCY_REQUIREMENTS)\n",
    "    \n",
    "    print(\"Saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "networks",
   "language": "python",
   "name": "networks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
